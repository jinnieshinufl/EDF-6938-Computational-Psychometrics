{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530144c4",
   "metadata": {},
   "source": [
    "<h1><center>  lab 3 : ML Overview: Supervised Learning algorithms </center>\n",
    "    \n",
    "<img src=\"https://i.pinimg.com/736x/02/66/bd/0266bd22348df67f4cf04ab83bf38e5a.jpg\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7066e",
   "metadata": {},
   "source": [
    "```Created by Jinnie Shin (jinnie.shin@coe.ufl.edu)```\\\n",
    "```Date: May 19th 2022```\n",
    "\n",
    "```Image source: https://i.pinimg.com/736x/02/66/bd/0266bd22348df67f4cf04ab83bf38e5a.jpg```\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQmNf86oJnfhpkPA9LnrFnAbfwF2VywPYpB_w&usqp=CAU\" align=\"left\" width=\"70\" height=\"70\" align=\"left\"> \n",
    "\n",
    " ### Required Packages or Dependencies\n",
    " We learned how to download packages in python in our last lecture -- e.g., pip install `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02eac3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install { } ! in case you run into the `package not avaialble` error\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "#import matplotlib.pyplot as plt ### ----- pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8867d2",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data consisted of the labeled chats from an online collaborative problem-solving study (Hao et al., 2015). Each turn of the chats has been coded into **one of the four categories** of collaborative problem-solving skills (Link: https://repository.isls.org/bitstream/1/462/1/297.pdf)\n",
    "- *Macro level evience* - infer: Collaborative problem-solving skills\n",
    "\n",
    "#### Goal\n",
    "- To create an automated classifier using machine learning methods to label the chats automatically\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13a38ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_X=pd.read_csv('./week3_data/chat_bigram_feature.csv.gz',compression='gzip').values\n",
    "real_y=pd.read_csv('./week3_data/chat_label.csv').values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9abed1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116c6dd5",
   "metadata": {},
   "source": [
    "- Each turn of the chats is already converted into a numerical vector using a NLP vectorization approach (we will cover this in our NLP class :)) \n",
    "- So now one variable (or feature in CP) represents a uni-gram or a bi-gram token (e.g., \"I\", \"I-HAVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ebf4979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 3, 3, 4])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_y # Let's take a look at the output data together "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d8cb2",
   "metadata": {},
   "source": [
    "- Like we explained earlier, the output variable is the hand-coded categorization of one of the four collaborative problem-solving skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee132932",
   "metadata": {},
   "source": [
    "## 1. Regression OR Classification Problems?\n",
    "\n",
    "> Our task is to predict the `output categories --y`  using the given set of features `X`. \n",
    "\n",
    "### Problem setting 1\n",
    "> Let's assume that the complexity of the skill dimension is the **lowest in category 1** and the **highest in category 4**\n",
    "\n",
    "$\\rightarrow$ Predict whether the chat can be used to automatically predict the complexity of the problem solving skills\n",
    "\n",
    "> We will implement and use a linear regression model, as our main prediction. We will take a look at how the model weights are learned using **the gradient descent algorithms**. \n",
    "\n",
    "### 1.1 Gradient Descent \n",
    "<img src=\"https://miro.medium.com/proxy/1*fBxEzbzP1KkqR7PTexJZdw.png\" width=\"250\">\n",
    "\n",
    "> The objective of the learning algorithm is to determine the best possible values for the parameters (`w` and `b`), such that the overall loss (squared error loss) of the model is minimized as much as possible. \\\n",
    "> Let's solve this regression problem: `y = 4.0+(3.0洧논0)+(1.0洧논1)+(3.0洧논2)+(0.5洧논3)+(1.5洧논4)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e5ad077",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 200\n",
    "\n",
    "x0 = 3.0 + np.random.standard_normal(num_samples)\n",
    "x1 = 1.0 + np.random.standard_normal(num_samples)\n",
    "x2 = -8.0 + np.random.standard_normal(num_samples)\n",
    "x3 = -2.0 + np.random.standard_normal(num_samples)\n",
    "x4 = 0.5 + np.random.standard_normal(num_samples)\n",
    "y = 4.0 + 3.0 * x0 + 1.0 * x1 + 3.0 * x2 + 0.5 * x3 + 1.5 * x4 + np.random.standard_normal(num_samples)\n",
    "\n",
    "X = np.column_stack((x0, x1, x2, x3, x4))\n",
    "Y = y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938556da",
   "metadata": {},
   "source": [
    "#### 1.1.1 Batch Gradient Descent (BGD)\n",
    "source: https://www.youtube.com/watch?v=b4Vyma9wPHo \n",
    "<img src=\"https://i.ytimg.com/vi/b4Vyma9wPHo/maxresdefault.jpg\" width='500'>\n",
    "\n",
    "\n",
    "> Partial derivates of `b` and `w` in linear regression with the squared loss is: \n",
    "<img src=\"https://eli.thegreenplace.net/images/math/aef02f077919896478d0456619f934dcc5809142.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c448b8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3453809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGD(X, Y, b, w, alpha=0.005): # alpha is a learning rate, we will set it as 0.005 for now\n",
    "   \n",
    "    num_feat = X.shape[1]\n",
    "    \n",
    "    num_sample = X.shape[0] # This indicates the total number of data points (rows)\n",
    "\n",
    "    b_grad = 0 #Intercept \n",
    "    \n",
    "    w_grad = np.zeros(num_feat) # weight vector \n",
    "    \n",
    "    for i in range(num_sample): # BGD first calculates the `b_grad` or `w_grad` \n",
    "                                # from the total sample N\n",
    "        y = Y[i] # one sample, y\n",
    "        x = X[i] # one sample, x \n",
    "        b_grad += -(2./float(num_sample)) * (y - (b + w.dot(x)))\n",
    "\n",
    "        for j in range(num_feat):\n",
    "            x_ij = x[j]\n",
    "            w_grad[j] += -(2./float(num_sample)) * x_ij * (y - (b + w.dot(x)))\n",
    "\n",
    "    b_new = b - alpha * b_grad\n",
    "    w_new = np.array([w[i] - alpha * w_grad[i] for i in range(num_feat)])\n",
    "    return b_new, w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dccd0db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGD_train(X, Y, alpha=0.005):\n",
    "    b = 0\n",
    "    w = np.zeros(X.shape[1])\n",
    "    print('===== Start Training ====')\n",
    "    for i in range(100000):\n",
    "        b_new, w_new = BGD(X, Y, b, w, alpha=alpha)\n",
    "        b = b_new\n",
    "        w = w_new\n",
    "        if i % 5000 == 0:\n",
    "            print('{}: b = {}, w = {}'.format(i, np.round(b_new, 2), np.round(w_new, 2)))\n",
    "\n",
    "    print('final: b = {}, w = {}'.format(np.round(b, 2), np.round(w, 2)))\n",
    "    return b, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d82dc5",
   "metadata": {},
   "source": [
    "> *Let's explore!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29b5e930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Start Training ====\n",
      "0: b = -0.1, w = [-0.28 -0.09  0.83  0.22 -0.04]\n",
      "5000: b = 1.7, w = [2.94 0.93 2.69 0.48 1.61]\n",
      "10000: b = 2.84, w = [2.91 0.91 2.8  0.51 1.61]\n",
      "15000: b = 3.54, w = [2.89 0.9  2.87 0.53 1.6 ]\n",
      "20000: b = 3.97, w = [2.87 0.9  2.92 0.55 1.6 ]\n",
      "25000: b = 4.23, w = [2.87 0.9  2.94 0.55 1.6 ]\n",
      "30000: b = 4.39, w = [2.86 0.89 2.96 0.56 1.6 ]\n",
      "35000: b = 4.49, w = [2.86 0.89 2.97 0.56 1.59]\n",
      "40000: b = 4.55, w = [2.86 0.89 2.98 0.56 1.59]\n",
      "45000: b = 4.59, w = [2.85 0.89 2.98 0.56 1.59]\n",
      "50000: b = 4.61, w = [2.85 0.89 2.98 0.56 1.59]\n",
      "55000: b = 4.62, w = [2.85 0.89 2.98 0.56 1.59]\n",
      "60000: b = 4.63, w = [2.85 0.89 2.98 0.56 1.59]\n",
      "65000: b = 4.64, w = [2.85 0.89 2.99 0.56 1.59]\n",
      "70000: b = 4.64, w = [2.85 0.89 2.99 0.56 1.59]\n",
      "75000: b = 4.64, w = [2.85 0.89 2.99 0.57 1.59]\n",
      "80000: b = 4.64, w = [2.85 0.89 2.99 0.57 1.59]\n",
      "85000: b = 4.64, w = [2.85 0.89 2.99 0.57 1.59]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fb/h76bxzm91r32gmwmtqr_zgvm0000gn/T/ipykernel_67210/3902082591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBGD_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fb/h76bxzm91r32gmwmtqr_zgvm0000gn/T/ipykernel_67210/2859009555.py\u001b[0m in \u001b[0;36mBGD_train\u001b[0;34m(X, Y, alpha)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'===== Start Training ===='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mb_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fb/h76bxzm91r32gmwmtqr_zgvm0000gn/T/ipykernel_67210/3034364791.py\u001b[0m in \u001b[0;36mBGD\u001b[0;34m(X, Y, b, w, alpha)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mx_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mw_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_ij\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mb_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BGD_train(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6db121",
   "metadata": {},
   "source": [
    "<img src=\"https://i.pinimg.com/736x/2e/aa/7d/2eaa7d5021ca7c3c98bc93b98b9646fe.jpg\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n",
    "\n",
    " ## Task 1: Training & Testing data\n",
    ">  **Q1.** In order to analyze large dataset efficiently, we will use the package `scikit-learn` to implement regression models. \n",
    ">> **Step 1**: Download the package `!pip install sklearn` \\\n",
    ">> **Step 2**: Import models ` from sklearn.linear_model import LinearRegression`\\\n",
    ">> **Step 3**: Call the module `lr = LinearRegression()` \\\n",
    ">> **Step 4**: Fit the dataset using `lr.fit({input}, {output})` and check the intercept and the coefficients using `lr.intercept_` and `lr.coef_`\n",
    "\n",
    "> More information about the package is available at: https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares\n",
    "\n",
    "> **Q2.** Compare the results with our findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7f0c1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the intercept value of my linear regression -------\n",
      "4.6452278156334135\n",
      "This is the coefficient value of my linear regression -------\n",
      "[2.85291625 0.89002873 2.98601052 0.56510493 1.59376851]\n"
     ]
    }
   ],
   "source": [
    "################################### YOUR CODE HERE #############################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "lr = LinearRegression() # model development \n",
    "\n",
    "lr.fit(X, y) #model learning \n",
    "\n",
    "y_hat = lr.predict(X)\n",
    "\n",
    "print('This is the intercept value of my linear regression -------')\n",
    "print(lr.intercept_)\n",
    "print('This is the coefficient value of my linear regression -------')\n",
    "print(lr.coef_)\n",
    "\n",
    "\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677dd035",
   "metadata": {},
   "source": [
    "<img src=\"https://i.pinimg.com/736x/2e/aa/7d/2eaa7d5021ca7c3c98bc93b98b9646fe.jpg\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n",
    "\n",
    " ## Task 2: Training & Testing data - using our real data\n",
    ">  Q3. Let's try to use our `real_X` and `real_y` to investigate whether the chat can sufficiently predict the complexity of the CPS skills. In order to analyze large dataset efficiently, we will use the package `scikit-learn` to implement regression models. \n",
    ">\n",
    ">> **Step 1**: Import models ` from sklearn.linear_model import LinearRegression`\\\n",
    ">> **Step 2**: Call a new module `lr = LinearRegression()` \\\n",
    ">> **Step 3**: Fit the dataset using `lr.fit({input}, {output})` and check the intercept and the coefficients using `lr.intercept_` and `lr.coef_`\\\n",
    ">> **Step 4**: Using the model you fit in Step 3, let's predict the outcome and round it up using `pred = lr.predict(real_X)`\\\n",
    ">> **Step 5**: Let's evaluate the prediction accuracy. First import `from sklearn.metrics import accuracy_score, r2_score`\n",
    "and going back to your script `r2_score(pred, {output})` `accuracy_score(pred, {output})`\n",
    "\n",
    "> More information about the package is available at: https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares\n",
    "\n",
    "> Q4. Any problems with this evaluation scheme?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d946cd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6402474636318217"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################### YOUR CODE HERE #############################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()  # step2: model development \n",
    "lr.fit(real_X, real_y)   # step3: model learning - fitting \n",
    "\n",
    "pred = lr.predict(real_X) # prediction \n",
    "\n",
    "from sklearn.metrics import accuracy_score, r2_score \n",
    "\n",
    "r2_score(pred, real_y) # step4: model evaluation \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f35a8c",
   "metadata": {},
   "source": [
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8328b",
   "metadata": {},
   "source": [
    "### Problem setting 2\n",
    "> Let's classify the lables using the chat \n",
    "\n",
    "$\\rightarrow$ classify the lables using the chat. \n",
    "\n",
    "> We will implement and use the support vector classifer and neural networks model (what we covered in the previous lecture :)) ), as our main prediction. We will take a look at how the model weights are learned using **the gradient descent algorithms**. \n",
    "\n",
    "### We will first define a few models we learned so far... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85ec37ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC # for Support Vector Machine\n",
    "model_SVM = LinearSVC()\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier #neural network \n",
    "nn = MLPClassifier()\n",
    "# let's take a look: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc2b408",
   "metadata": {},
   "source": [
    "> *Let's explore!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd8ace03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration--- 1\n",
      "0.7215474583895637\n",
      "iteration--- 2\n",
      "0.6842105263157895\n",
      "iteration--- 3\n",
      "0.7125506072874493\n",
      "iteration--- 4\n",
      "0.7151215121512151\n",
      "iteration--- 5\n",
      "0.666966696669667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC # for Support Vector Machine\n",
    "from sklearn.metrics import accuracy_score # metric\n",
    "from sklearn.model_selection import KFold  # evaluation scheme\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "i=0\n",
    "for train_index, test_index  in kf.split(real_X):\n",
    "    \n",
    "    X_train, X_test = real_X[train_index], real_X[test_index] #this is to find and define train-test-folds\n",
    "    y_train, y_test = real_y[train_index], real_y[test_index]\n",
    "\n",
    "    model_SVM = LinearSVC() #this is to define or develop a model \n",
    "    model_SVM.fit(X_train, y_train) #step 3 : model learning but only using the training set \n",
    "    pred = model_SVM.predict(X_test)\n",
    "    print('iteration---', i+1)\n",
    "    print(accuracy_score(pred, y_test))\n",
    "    i+=1 \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c646f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
