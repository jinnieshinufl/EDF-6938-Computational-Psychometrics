{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530144c4",
   "metadata": {},
   "source": [
    "<h1><center>  lab 3 : ML Overview: Supervised Learning algorithms </center>\n",
    "    \n",
    "<img src=\"https://i.pinimg.com/736x/02/66/bd/0266bd22348df67f4cf04ab83bf38e5a.jpg\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7066e",
   "metadata": {},
   "source": [
    "```Created by Jinnie Shin (jinnie.shin@coe.ufl.edu)```\\\n",
    "```Date: May 19th 2022```\n",
    "\n",
    "```Image source: https://i.pinimg.com/736x/02/66/bd/0266bd22348df67f4cf04ab83bf38e5a.jpg```\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQmNf86oJnfhpkPA9LnrFnAbfwF2VywPYpB_w&usqp=CAU\" align=\"left\" width=\"70\" height=\"70\" align=\"left\"> \n",
    "\n",
    " ### Required Packages or Dependencies\n",
    " We learned how to download packages in python in our last lecture -- e.g., pip install `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02eac3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install { } ! in case you run into the `package not avaialble` error\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt ### ----- pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8867d2",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data consisted of the labeled chats from an online collaborative problem-solving study (Hao et al., 2015). Each turn of the chats has been coded into **one of the four categories** of collaborative problem-solving skills (Link: https://repository.isls.org/bitstream/1/462/1/297.pdf)\n",
    "- *Macro level evience* - infer: Collaborative problem-solving skills\n",
    "\n",
    "#### Goal\n",
    "- To create an automated classifier using machine learning methods to label the chats automatically\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13a38ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_X=pd.read_csv('./week3_data/chat_bigram_feature.csv.gz',compression='gzip').values\n",
    "real_y=pd.read_csv('./week3_data/chat_label.csv').values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9abed1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11113, 3222)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_X.shape # Let's take a look at the shape of the input data together "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116c6dd5",
   "metadata": {},
   "source": [
    "- Each turn of the chats is already converted into a numerical vector using a NLP vectorization approach (we will cover this in our NLP class :)) \n",
    "- So now one variable (or feature in CP) represents a uni-gram or a bi-gram token (e.g., \"I\", \"I-HAVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ebf4979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 3, 3, 4])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_y # Let's take a look at the output data together "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d8cb2",
   "metadata": {},
   "source": [
    "- Like we explained earlier, the output variable is the hand-coded categorization of one of the four collaborative problem-solving skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee132932",
   "metadata": {},
   "source": [
    "## 1. Regression OR Classification Problems?\n",
    "\n",
    "> Our task is to predict the `output categories --y`  using the given set of features `X`. \n",
    "\n",
    "### Problem setting 1\n",
    "> Let's assume that the complexity of the skill dimension is the **lowest in category 1** and the **highest in category 4**\n",
    "\n",
    "$\\rightarrow$ Predict whether the chat can be used to automatically predict the complexity of the problem solving skills\n",
    "\n",
    "> We will implement and use a linear regression model, as our main prediction. We will take a look at how the model weights are learned using **the gradient descent algorithms**. \n",
    "\n",
    "### 1.1 Gradient Descent \n",
    "<img src=\"https://miro.medium.com/proxy/1*fBxEzbzP1KkqR7PTexJZdw.png\" width=\"250\">\n",
    "\n",
    "> The objective of the learning algorithm is to determine the best possible values for the parameters (`w` and `b`), such that the overall loss (squared error loss) of the model is minimized as much as possible. \\\n",
    "> Let's solve this regression problem: `y = 4.0+(3.0洧논0)+(1.0洧논1)+(3.0洧논2)+(0.5洧논3)+(1.5洧논4)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e5ad077",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 200\n",
    "\n",
    "x0 = 3.0 + np.random.standard_normal(num_samples)\n",
    "x1 = 1.0 + np.random.standard_normal(num_samples)\n",
    "x2 = -8.0 + np.random.standard_normal(num_samples)\n",
    "x3 = -2.0 + np.random.standard_normal(num_samples)\n",
    "x4 = 0.5 + np.random.standard_normal(num_samples)\n",
    "y = 4.0 + 3.0 * x0 + 1.0 * x1 + 3.0 * x2 + 0.5 * x3 + 1.5 * x4 + np.random.standard_normal(num_samples)\n",
    "\n",
    "X = np.column_stack((x0, x1, x2, x3, x4))\n",
    "Y = y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938556da",
   "metadata": {},
   "source": [
    "#### 1.1.1 Batch Gradient Descent (BGD)\n",
    "> Partial derivates of `b` and `w` in linear regression with the squared loss is: \n",
    "<img src=\"https://eli.thegreenplace.net/images/math/aef02f077919896478d0456619f934dcc5809142.png\" width=\"250\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3453809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGD(X, Y, b, w, alpha=0.005): # alpha is a learning rate, we will set it as 0.005 for now\n",
    "   \n",
    "    num_feat = X.shape[1]\n",
    "    \n",
    "    num_sample = X.shape[0] # This indicates the total number of data points (rows)\n",
    "\n",
    "    b_grad = 0 #Intercept \n",
    "    \n",
    "    w_grad = np.zeros(num_feat) # weight vector \n",
    "    \n",
    "    for i in range(num_sample): # BGD first calculates the `b_grad` or `w_grad` \n",
    "                                # from the total sample N\n",
    "        y = Y[i] # one sample, y\n",
    "        x = X[i] # one sample, x \n",
    "        b_grad += -(2./float(num_sample)) * (y - (b + w.dot(x)))\n",
    "\n",
    "        for j in range(num_feat):\n",
    "            x_ij = x[j]\n",
    "            w_grad[j] += -(2./float(num_sample)) * x_ij * (y - (b + w.dot(x)))\n",
    "\n",
    "    b_new = b - alpha * b_grad\n",
    "    w_new = np.array([w[i] - alpha * w_grad[i] for i in range(num_feat)])\n",
    "    return b_new, w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dccd0db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGD_train(X, Y, alpha=0.005):\n",
    "    b = 0\n",
    "    w = np.zeros(X.shape[1])\n",
    "    print('===== Start Training ====')\n",
    "    for i in range(100000):\n",
    "        b_new, w_new = BGD(X, Y, b, w, alpha=alpha)\n",
    "        b = b_new\n",
    "        w = w_new\n",
    "        if i % 5000 == 0:\n",
    "            print('{}: b = {}, w = {}'.format(i, np.round(b_new, 2), np.round(w_new, 2)))\n",
    "\n",
    "    print('final: b = {}, w = {}'.format(np.round(b, 2), np.round(w, 2)))\n",
    "    return b, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d82dc5",
   "metadata": {},
   "source": [
    "> *Let's explore!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29b5e930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Start Training ====\n",
      "0: b = -0.1, w = [-0.28 -0.09  0.85  0.2  -0.04]\n",
      "5000: b = 1.57, w = [3.14 0.93 2.78 0.42 1.51]\n",
      "10000: b = 2.57, w = [3.1  0.92 2.88 0.44 1.49]\n",
      "15000: b = 3.17, w = [3.08 0.9  2.94 0.46 1.48]\n",
      "20000: b = 3.52, w = [3.07 0.9  2.97 0.47 1.48]\n",
      "25000: b = 3.72, w = [3.07 0.89 2.99 0.47 1.48]\n",
      "30000: b = 3.85, w = [3.06 0.89 3.   0.47 1.47]\n",
      "35000: b = 3.92, w = [3.06 0.89 3.01 0.48 1.47]\n",
      "40000: b = 3.96, w = [3.06 0.89 3.02 0.48 1.47]\n",
      "45000: b = 3.99, w = [3.06 0.89 3.02 0.48 1.47]\n",
      "50000: b = 4.0, w = [3.06 0.89 3.02 0.48 1.47]\n",
      "55000: b = 4.01, w = [3.06 0.89 3.02 0.48 1.47]\n",
      "60000: b = 4.02, w = [3.06 0.89 3.02 0.48 1.47]\n",
      "65000: b = 4.02, w = [3.06 0.89 3.02 0.48 1.47]\n",
      "70000: b = 4.02, w = [3.06 0.89 3.02 0.48 1.47]\n",
      "75000: b = 4.02, w = [3.06 0.89 3.02 0.48 1.47]\n",
      "80000: b = 4.03, w = [3.06 0.89 3.02 0.48 1.47]\n",
      "85000: b = 4.03, w = [3.06 0.89 3.02 0.48 1.47]\n",
      "90000: b = 4.03, w = [3.06 0.89 3.02 0.48 1.47]\n",
      "95000: b = 4.03, w = [3.06 0.89 3.02 0.48 1.47]\n",
      "final: b = 4.03, w = [3.06 0.89 3.02 0.48 1.47]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.026482762169681,\n",
       " array([3.05604783, 0.88702139, 3.02165789, 0.47876374, 1.47200593]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BGD_train(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6db121",
   "metadata": {},
   "source": [
    "<img src=\"https://i.pinimg.com/736x/2e/aa/7d/2eaa7d5021ca7c3c98bc93b98b9646fe.jpg\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n",
    "\n",
    " ## Task 1: Training & Testing data\n",
    ">  **Q1.** In order to analyze large dataset efficiently, we will use the package `scikit-learn` to implement regression models. \n",
    ">> **Step 1**: Download the package `!pip install sklearn` \\\n",
    ">> **Step 2**: Import models ` from sklearn.linear_model import LinearRegression`\\\n",
    ">> **Step 3**: Call the module `lr = LinearRegression()` \\\n",
    ">> **Step 4**: Fit the dataset using `lr.fit({input}, {output})` and check the intercept and the coefficients using `lr.intercept_` and `lr.coef_`\n",
    "\n",
    "> More information about the package is available at: https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares\n",
    "\n",
    "> **Q2.** Compare the results with our findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7f0c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### YOUR CODE HERE #############################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677dd035",
   "metadata": {},
   "source": [
    "<img src=\"https://i.pinimg.com/736x/2e/aa/7d/2eaa7d5021ca7c3c98bc93b98b9646fe.jpg\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n",
    "\n",
    " ## Task 2: Training & Testing data - using our real data\n",
    ">  Q3. Let's try to use our `real_X` and `real_y` to investigate whether the chat can sufficiently predict the complexity of the CPS skills. In order to analyze large dataset efficiently, we will use the package `scikit-learn` to implement regression models. \n",
    ">\n",
    ">> **Step 1**: Import models ` from sklearn.linear_model import LinearRegression`\\\n",
    ">> **Step 2**: Call a new module `lr = LinearRegression()` \\\n",
    ">> **Step 3**: Fit the dataset using `lr.fit({input}, {output})` and check the intercept and the coefficients using `lr.intercept_` and `lr.coef_`\\\n",
    ">> **Step 4**: Using the model you fit in Step 3, let's predict the outcome and round it up using `pred = lr.predict(real_X)`\\\n",
    ">> **Step 5**: Let's evaluate the prediction accuracy. First import `from sklearn.metrics import accuracy_score, r2_score`\n",
    "and going back to your script `r2_score(pred, {output})` `accuracy_score(pred, {output})`\n",
    "\n",
    "> More information about the package is available at: https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares\n",
    "\n",
    "> Q4. Any problems with this evaluation scheme?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d946cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### YOUR CODE HERE #############################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f35a8c",
   "metadata": {},
   "source": [
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8328b",
   "metadata": {},
   "source": [
    "### Problem setting 2\n",
    "> Let's classify the lables using the chat \n",
    "\n",
    "$\\rightarrow$ classify the lables using the chat. \n",
    "\n",
    "> We will implement and use the support vector classifer and neural networks model (what we covered in the previous lecture :)) ), as our main prediction. We will take a look at how the model weights are learned using **the gradient descent algorithms**. \n",
    "\n",
    "### We will first define a few models we learned so far... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85ec37ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC # for Support Vector Machine\n",
    "model_SVM = LinearSVC()\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier #neural network \n",
    "nn = MLPClassifier()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc2b408",
   "metadata": {},
   "source": [
    "> *Let's explore!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4fd698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
